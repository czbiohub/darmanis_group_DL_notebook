{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# manually curated plates list\n",
    "all_plates = ['B000771',\n",
    " 'B000873',\n",
    " 'B001429',\n",
    " 'B001438',\n",
    " 'B001439',\n",
    " 'B001440',\n",
    " 'B001476',\n",
    " 'B001561',\n",
    " 'B001564',\n",
    " 'B001566',\n",
    " 'B002081',\n",
    " 'B002083',\n",
    " 'B002343',\n",
    " 'B002522',\n",
    " 'B002524',\n",
    " 'B002530',\n",
    " 'B002549',\n",
    " 'B002550',\n",
    " 'B003063',\n",
    " 'B003096',\n",
    " 'B003104',\n",
    " 'B003198',\n",
    " 'B003207',\n",
    " 'B003346',\n",
    " 'B003469',\n",
    " 'B003520',\n",
    " 'B003526',\n",
    " 'B003580',\n",
    " 'B003590',\n",
    " 'B003591',\n",
    " 'B003593',\n",
    " 'B003595',\n",
    " 'B003605',\n",
    " 'B003817',\n",
    " 'B003819',\n",
    " 'B003823',\n",
    " 'B003828',\n",
    " 'B003829',\n",
    " 'B003830',\n",
    " 'B003832',\n",
    " 'B003916']\n",
    "\n",
    "cores = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import boto3\n",
    "import botocore.exceptions\n",
    "import re, os, shutil\n",
    "\n",
    "def get_s3path_list(bucket, prefix, suffix):\n",
    "    #     bucket = 'darmanis-group'\n",
    "    #     prefix = 'singlecell_lungadeno/rawdata/fastqs'\n",
    "    #     suffix = 'fastq.gz'\n",
    "\n",
    "    client = boto3.client('s3')\n",
    "    paginator = client.get_paginator('list_objects')\n",
    "    operation_parameters = {'Bucket': bucket,\n",
    "                            'Prefix': prefix}\n",
    "    page_iterator = paginator.paginate(**operation_parameters)\n",
    "    paths = ['s3://{}/{}'.format(bucket, key['Key']) for page in page_iterator for key in page['Contents'] if key['Key'].endswith(suffix)]\n",
    "    return paths\n",
    "\n",
    "def s3_restore(args):\n",
    "    b,k = args\n",
    "    # b = bucket name\n",
    "    # k = file key\n",
    "    obj = s3r.Object(b, k)\n",
    "    storage_class = obj.storage_class\n",
    "    restore = obj.restore\n",
    "    if not obj.restore:\n",
    "        resp = bucket.meta.client.restore_object(\n",
    "            Bucket=b,\n",
    "            Key=k,\n",
    "            RestoreRequest={'Days': 3}\n",
    "        )\n",
    "        restore = f'{b}/{k}'\n",
    "    return restore\n",
    "    \n",
    "def s3_restore_parallel(b,k_list,n_proc=2):\n",
    "\n",
    "    global s3r\n",
    "    s3r = boto3.resource('s3')\n",
    "    global bucket\n",
    "    bucket = s3r.Bucket(b)\n",
    "\n",
    "    print('creating pool')\n",
    "\n",
    "    p = multiprocessing.Pool(processes=n_proc)\n",
    "\n",
    "    try:\n",
    "        print('restoring files...')\n",
    "        output = p.map(s3_restore, zip([b] * len(k_list), k_list), chunksize=100)\n",
    "    finally:\n",
    "        p.close()\n",
    "        p.join()\n",
    "        \n",
    "    return output\n",
    "\n",
    "def s3copy(args):\n",
    "    \"\"\"return failed paths\"\"\"\n",
    "    src, dest = args\n",
    "    s3cmd = f'aws s3 cp {src} {dest}'.split(' ')\n",
    "    process = subprocess.run(s3cmd)\n",
    "    if process.returncode == 0:\n",
    "        path = None\n",
    "    else:\n",
    "        path = src\n",
    "    return src\n",
    "\n",
    "def s3copy_parallel(src_list, dest_list, n_proc = 2):\n",
    "    try:\n",
    "        p = multiprocessing.Pool(processes=n_proc)\n",
    "        process = p.map(s3copy, zip(src_list, dest_list), chunksize=100)\n",
    "    finally:\n",
    "        p.close()\n",
    "        p.join()\n",
    "    return process\n",
    "\n",
    "def s3move(args):\n",
    "    \"\"\"return failed paths\"\"\"\n",
    "    src, dest = args\n",
    "    s3cmd = f'aws s3 mv {src} {dest}'.split(' ')\n",
    "    process = subprocess.run(s3cmd)\n",
    "    if process.returncode == 0:\n",
    "        path = None\n",
    "    else:\n",
    "        path = src\n",
    "    return src\n",
    "\n",
    "def s3move_parallel(src_list, dest_list, n_proc = 2):\n",
    "    try:\n",
    "        p = multiprocessing.Pool(processes=n_proc)\n",
    "        process = p.map(s3move, zip(src_list, dest_list), chunksize=100)\n",
    "    finally:\n",
    "        p.close()\n",
    "        p.join()\n",
    "    return process\n",
    "\n",
    "def s3rm(src):\n",
    "    syscmd = f'aws s3 rm {src}'\n",
    "    process = subprocess.run(syscmd.split(' '))\n",
    "    if process.returncode == 0:\n",
    "        path = None\n",
    "    else:\n",
    "        path = src\n",
    "    return src\n",
    "\n",
    "def s3rm_parallel(paths, n_proc = 2):\n",
    "    try:\n",
    "        p = multiprocessing.Pool(processes=n_proc)\n",
    "        process = p.map(s3rm, paths, chunksize=100)\n",
    "    finally:\n",
    "        p.close()\n",
    "        p.join()\n",
    "    return process\n",
    "\n",
    "def merge_counts(top_dir):\n",
    "    # Create big counts table from local tables\n",
    "    file_list = [filename for filename in glob.iglob(top_dir + '**/*.txt', recursive=True)]\n",
    "    first_df = pd.read_csv(file_list[0], header=None, delimiter='\\t')\n",
    "    num_row = len(first_df)\n",
    "    rownames = first_df.iloc[:,0].tolist()\n",
    "    num_col = len(file_list)\n",
    "    colnames = []\n",
    "    empty_array = np.zeros((num_row, num_col))\n",
    "    \n",
    "    for idx, file in tqdm.tqdm(enumerate(file_list)):\n",
    "        pulled_col = pd.read_csv(file, header=None, delimiter='\\t', usecols=[1])\n",
    "        colname = '_'.join(file.split('/')[-1].split('_')[:2] + ['0'])\n",
    "        if colname in colnames:\n",
    "            name_split = colname.split('_')\n",
    "            new_idx =  int(name_split[-1]) + 1\n",
    "            colname = '_'.join(name_split[:2] + [new_idx])\n",
    "        colnames.append(colname)\n",
    "        empty_array[:,idx] = pulled_col.values.reshape((len(pulled_col),))\n",
    "    \n",
    "    # convert numpy to pandas\n",
    "    master_df = pd.DataFrame(empty_array)\n",
    "    master_df.columns = colnames\n",
    "    master_df['gene'] = rownames\n",
    "    \n",
    "    # remove metadata \n",
    "    master_df = master_df[[\"__\" not in x for x in master_df.gene]]\n",
    "    \n",
    "    # reset gene col\n",
    "    master_df = master_df.set_index('gene').reset_index()\n",
    "    \n",
    "    return master_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parse s3 file system\n",
    "# czb_paths = get_s3path_list('czb-seqbot', 'fastqs', 'homo.htseq-count.txt')\n",
    "# czbiohub_paths = get_s3path_list('czbiohub-seqbot', 'fastqs', 'homo.htseq-count.txt')\n",
    "# darmanis_paths = get_s3path_list('darmanis-group', 'melanocyte_primary', 'homo.htseq-count.txt')\n",
    "# all_paths = czb_paths + czbiohub_paths + darmanis_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse s3 file system\n",
    "suffix_oi = '.homo.Aligned.out.sorted.bam'\n",
    "czb_paths = get_s3path_list('czb-seqbot', 'fastqs', suffix_oi)\n",
    "czbiohub_paths = get_s3path_list('czbiohub-seqbot', 'fastqs', suffix_oi)\n",
    "darmanis_paths = get_s3path_list('darmanis-group', 'melanocyte_primary', suffix_oi)\n",
    "all_paths = czb_paths + czbiohub_paths + darmanis_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# extract well-plate information from cell-gene counts tables\n",
    "dest_prefix = 's3://darmanis-group/melanocyte_primary/melanocyte_primary/bams'\n",
    "\n",
    "paths_df = pd.DataFrame({'path':[x for x in all_paths if any([y in x for y in all_plates])]})\n",
    "paths_df['well'] = [x.split('/')[-1].split('_')[0] for x in paths_df['path']] \n",
    "paths_df['plate'] = [x.split('/')[-1].split('_')[1] for x in paths_df['path']] \n",
    "paths_df['bucket'] = [x.split('/')[2] for x in paths_df['path']]\n",
    "paths_df['key'] = ['/'.join(x.split('/')[3:]) for x in paths_df['path']]\n",
    "paths_df['dest'] = [f'{dest_prefix}/{x}_{y}.bam' for x,y in zip(paths_df.well.values.tolist(),\n",
    "                            paths_df.plate.values.tolist())]\n",
    "\n",
    "paths_df.to_csv('~/data/DL20181011_melanocyte_test_data/bams_table.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # restore files that have been glacier-ized\n",
    "# for b in set(paths_df.bucket):\n",
    "#     print('restoring', b)\n",
    "#     df_slice = paths_df[paths_df.bucket == b]\n",
    "#     k_list = df_slice.key.values.tolist()\n",
    "#     output = s3_restore_parallel(b,k_list,n_proc=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy files\n",
    "## Check if file already exists in destination\n",
    "dest_paths = get_s3path_list('darmanis-group', \n",
    "                             'melanocyte_primary/melanocyte_primary/bams', \n",
    "                             '.bam')\n",
    "\n",
    "## update transfer lists\n",
    "src_list = paths_df[[x not in dest_paths for x in paths_df.dest]].path.values.tolist()\n",
    "dest_list = paths_df[[x not in dest_paths for x in paths_df.dest]].dest.values.tolist()\n",
    "\n",
    "## perform copy\n",
    "if len(src_list) > 0:\n",
    "    output = s3copy_parallel(src_list, dest_list, n_proc = cores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # copy files from s3 bucket to compute destination\n",
    "# ## pull file list to copy\n",
    "# dest_paths = get_s3path_list('darmanis-group', \n",
    "#                              'melanocyte_primary/melanocyte_primary/counts_tables', \n",
    "#                              '.txt')\n",
    "\n",
    "# ## copy files\n",
    "# local_wkdir = '/where/to/store/tmp'\n",
    "# local_dest_list = ['{}/{}'.format(local_wkdir, x.split('/')[-1]) for x in dest_paths]\n",
    "# output = s3copy_parallel(dest_paths, local_dest_list, n_proc = cores)\n",
    "\n",
    "# ## run merge\n",
    "# merged_df = merge_counts(local_wkdir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
