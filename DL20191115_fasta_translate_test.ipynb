{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os, subprocess, mygene, glob, tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def translate(seq, frame=1): \n",
    "    # frame: 1 = start at pos 0; 2 = start at pos 1; 3 = start at pos 2\n",
    "    table = { \n",
    "        'ATA':'I', 'ATC':'I', 'ATT':'I', 'ATG':'M', \n",
    "        'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T', \n",
    "        'AAC':'N', 'AAT':'N', 'AAA':'K', 'AAG':'K', \n",
    "        'AGC':'S', 'AGT':'S', 'AGA':'R', 'AGG':'R',                  \n",
    "        'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L', \n",
    "        'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P', \n",
    "        'CAC':'H', 'CAT':'H', 'CAA':'Q', 'CAG':'Q', \n",
    "        'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R', \n",
    "        'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V', \n",
    "        'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A', \n",
    "        'GAC':'D', 'GAT':'D', 'GAA':'E', 'GAG':'E', \n",
    "        'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G', \n",
    "        'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S', \n",
    "        'TTC':'F', 'TTT':'F', 'TTA':'L', 'TTG':'L', \n",
    "        'TAC':'Y', 'TAT':'Y', 'TAA':'', 'TAG':'', \n",
    "        'TGC':'C', 'TGT':'C', 'TGA':'', 'TGG':'W', \n",
    "    } \n",
    "    comp_dict = {'C':'G',\n",
    "                 'G':'C',\n",
    "                 'A':'T',\n",
    "                 'T':'A'\n",
    "                }\n",
    "    protein = ''\n",
    "    \n",
    "    if frame == 1 :\n",
    "        start_val = 0\n",
    "    if frame == 2:\n",
    "        start_val = 1\n",
    "    if frame == 3:\n",
    "        start_val = 2\n",
    "    if frame == 4 :\n",
    "        start_val = 0\n",
    "    if frame == 5:\n",
    "        start_val = 1\n",
    "    if frame == 6:\n",
    "        start_val = 2\n",
    "    if frame > 3:\n",
    "        seq = ''.join([comp_dict.get(x) for x in seq])\n",
    "        \n",
    "    for i in range(start_val, len(seq), 3): \n",
    "        try:\n",
    "            codon = seq[i:i + 3] \n",
    "            protein+= table[codon] \n",
    "        except:\n",
    "            break\n",
    "    return protein\n",
    "\n",
    "def split_translation (full_seq, len_cutoff = 10):\n",
    "    sub_seq = [x for x in full_seq.split('_') if len(x) > len_cutoff]\n",
    "    return sub_seq\n",
    "\n",
    "def print_fasta (sub_list, prefix='test'):\n",
    "    for idx, entry in enumerate(sub_list):\n",
    "        print(f'>{prefix}_{idx}\\n{entry}')\n",
    "        \n",
    "def save_fasta (sub_list, prefix, output):\n",
    "    with open(output, 'w') as f:\n",
    "        for idx, entry in enumerate(sub_list):\n",
    "            line = f'>{prefix}_{idx}\\n{entry}\\n'\n",
    "            f.write(line)\n",
    "            \n",
    "def process_fasta (infile, frames, outfile):\n",
    "    with open(infile, 'r') as f:\n",
    "        sublines = []\n",
    "        entry_name = ''\n",
    "        for idx,line in enumerate(f):\n",
    "            line = line[:-1]\n",
    "            if line.startswith('>') and entry_name == '':\n",
    "                entry_name = line\n",
    "            if line.startswith('>') and entry_name != '':\n",
    "                if len(sublines) > 0:\n",
    "                    print_seq = ''.join(sublines)\n",
    "                    ### process entry seq\n",
    "                    with open(outfile, 'a') as outf:\n",
    "                        for frame in frames:\n",
    "                            full_seq = translate(print_seq, frame)\n",
    "                            outf.write(f'{entry_name}__fr_{frame}\\n{full_seq}\\n')\n",
    "                    ###\n",
    "                    sublines = []\n",
    "                entry_name = line\n",
    "            else:\n",
    "                sublines = sublines+[line]                          \n",
    "        if len(sublines) > 0:\n",
    "            print_seq = ''.join(sublines)\n",
    "            ### process entry seq\n",
    "            with open(outfile, 'a') as outf:\n",
    "                for frame in frames:\n",
    "                    full_seq = translate(print_seq, frame)\n",
    "                    outf.write(f'{entry_name}__fr_{frame}\\n{full_seq}\\n')\n",
    "            ###\n",
    "            sublines = []\n",
    "            \n",
    "def run_blast(target_path,\n",
    "              target_suffix,\n",
    "              frames,\n",
    "              proteome_ref,\n",
    "              results_dir,\n",
    "              n_threads,\n",
    "              evalue_cutoff\n",
    "             ):\n",
    "    # extract cell id\n",
    "    cell_id = target_path.split('/')[-1].split(target_suffix)[0]\n",
    "    \n",
    "    # create results subdir\n",
    "    try:\n",
    "        shutil.rmtree(f'{results_dir}{cell_id}')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir(f'{results_dir}{cell_id}')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # translate nucl to prot\n",
    "    process_fasta(f'{target_path}', \n",
    "                  frames,\n",
    "                  f'{results_dir}{cell_id}/{cell_id}_protein.fa'\n",
    "                 )\n",
    "\n",
    "    # run blast\n",
    "    blast_cmd_list = ['blastp',\n",
    "                      '-query',\n",
    "                      f'{results_dir}{cell_id}/{cell_id}_protein.fa',\n",
    "                      '-db',\n",
    "                      proteome_ref,\n",
    "                      '-task',\n",
    "                      'blastp-fast',\n",
    "                      '-outfmt',\n",
    "                      '\"6 qseqid sseqid evalue\"',\n",
    "                      '-max_target_seqs',\n",
    "                      '1',\n",
    "                      '-out',\n",
    "                      f'{results_dir}{cell_id}/{cell_id}_blastpOut.txt',\n",
    "                      '-num_threads',\n",
    "                      f'{n_threads}'\n",
    "                     ]\n",
    "    blast_cmd = ' '.join(blast_cmd_list)\n",
    "    print(blast_cmd)\n",
    "    subprocess.call(blast_cmd, shell=True)\n",
    "    \n",
    "    # process results\n",
    "    process_blastp_results(results_dir, cell_id, evalue_cutoff)\n",
    "\n",
    "def process_blastp_results(results_dir, cell_id, evalue_cutoff):\n",
    "    # ingest table\n",
    "    results = pd.read_csv(f'{results_dir}{cell_id}/{cell_id}_blastpOut.txt', \n",
    "                          sep = '\\t',\n",
    "                          header = None\n",
    "                         )\n",
    "    results.columns = ['qsid' , 'ssid', 'evalue']\n",
    "    # parse frame and ids\n",
    "    results['frame'] = [x.split('__')[-1][-1] for x in results['qsid']]\n",
    "    results['qsid'] = [x.split('__')[0] for x in results['qsid']]\n",
    "    results['uniprot_id'] = [x.split('|')[1] for x in results['ssid']]\n",
    "    results['common_name'] = [x.split('|')[2] for x in results['ssid']]\n",
    "    # return best-match frame\n",
    "    results = results.sort_values('evalue', ascending = True).groupby(['qsid']).head(1)\n",
    "    # print undetermined reads to disk\n",
    "    undetermined_reads = results[results.evalue >= evalue_cutoff]\n",
    "    undetermined_reads.to_csv(f'{results_dir}{cell_id}/{cell_id}_undeterminedReads.csv')\n",
    "    # filter results to significant matches\n",
    "    results = results[results.evalue < evalue_cutoff]\n",
    "    # count matches\n",
    "    results = pd.DataFrame(results['uniprot_id'].value_counts())\n",
    "    # rename uniprot to gene symbols\n",
    "    results = (results\n",
    "               .reset_index()\n",
    "               .rename(columns={'index':'uniprot',\n",
    "                                'uniprot_id':'count'\n",
    "                               })\n",
    "              )\n",
    "    # ingest uniprot to symbol df for lookup\n",
    "    scope2field_df = pd.read_csv('/home/ubuntu/data/longread/proteome/scope2field.csv',index_col = 0)\n",
    "    results = pd.merge(scope2field_df,\n",
    "                       results,\n",
    "                       'left',\n",
    "                       'uniprot'\n",
    "                      )\n",
    "    # sanitize table\n",
    "    results = results.drop('uniprot', axis=1)\n",
    "    results['count'] = (results['count']\n",
    "                        .replace(np.nan, 0)\n",
    "                        .astype(int)\n",
    "                        .dropna()\n",
    "                       )\n",
    "    # write to disk\n",
    "    results.to_csv(f'{results_dir}{cell_id}/{cell_id}_geneCounts.csv')\n",
    "\n",
    "# create results dir\n",
    "def create_results_dir(results_dir):\n",
    "    try:\n",
    "        shutil.rmtree(f'{results_dir}')\n",
    "        print('####### deleted existing results subdir')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.mkdir(f'{results_dir}')\n",
    "        print('####### created results subdir')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# function to create blast db and config\n",
    "def make_protein_db (ref_path):\n",
    "    cmd = f'makeblastdb -in {ref_path} -dbtype prot'\n",
    "    print(cmd)\n",
    "    subprocess.call(cmd, shell=True)\n",
    "    print('####### created reference blast database')\n",
    "\n",
    "# function to create name conversion look up table\n",
    "def make_scope2field_lookup (ref_path, scope, field, species, results_dir):\n",
    "    \n",
    "    # ingest uniprot proteome\n",
    "    uniprot_list = []\n",
    "    with open(ref_path, 'r') as f:\n",
    "        for idx,line in enumerate(f):\n",
    "            line = line[:-1]\n",
    "            if line.startswith('>'):\n",
    "                # extract uniprot id field\n",
    "                line_split = line.split('|')[1]\n",
    "                uniprot_list = uniprot_list+[line_split]\n",
    "\n",
    "    # query for gene symbols\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    scope2field = mg.querymany(uniprot_list, \n",
    "                                       scopes=scope, \n",
    "                                       fields=field, \n",
    "                                       species=species,\n",
    "                                       returnall=False,\n",
    "                                       as_dataframe=True\n",
    "                                      )\n",
    "\n",
    "    # filter and save results\n",
    "    scope2field = scope2field[scope2field.notfound != True]\n",
    "    scope2field = (scope2field\n",
    "                         .reset_index()\n",
    "                         .loc[:, ['query',field]]\n",
    "                         .rename(columns = {'query':scope})\n",
    "                         .dropna()\n",
    "                        )\n",
    "    scope2field.to_csv(f'{results_dir}scope2field.csv')\n",
    "    print('####### created scope-to-field look-up table')\n",
    "\n",
    "# main function to process dir of samples\n",
    "def process_dir_samples (dir_path, \n",
    "                         ref_path,\n",
    "                         target_suffix,\n",
    "                         frames,\n",
    "                         n_threads,\n",
    "                         evalue_cutoff,\n",
    "                         results_dir,\n",
    "                         scope, \n",
    "                         field, \n",
    "                         species\n",
    "                        ):\n",
    "    # create results dir\n",
    "    create_results_dir(results_dir)\n",
    "    \n",
    "    # create blast database\n",
    "    make_protein_db (ref_path)\n",
    "    \n",
    "    # create name lookup table\n",
    "    make_scope2field_lookup (ref_path, scope, field, species, results_dir)\n",
    "    \n",
    "    for sample in tqdm.tqdm(glob.glob(f'{dir_path}*{target_suffix}')):\n",
    "        print(f'####### processing {sample}')\n",
    "        run_blast(sample,\n",
    "                  target_suffix,\n",
    "                  frames,\n",
    "                  ref_path,\n",
    "                  results_dir,\n",
    "                  n_threads,\n",
    "                  evalue_cutoff\n",
    "                 )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### created results subdir\n",
      "makeblastdb -in /home/ubuntu/data/longread/proteome/UP000005640_9606.fasta -dbtype prot\n",
      "####### created reference blast database\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "querying 10001-11000...done.\n",
      "querying 11001-12000...done.\n",
      "querying 12001-13000...done.\n",
      "querying 13001-14000...done.\n",
      "querying 14001-15000...done.\n",
      "querying 15001-16000...done.\n",
      "querying 16001-17000...done.\n",
      "querying 17001-18000...done.\n",
      "querying 18001-19000...done.\n",
      "querying 19001-20000...done.\n",
      "querying 20001-20659...done.\n"
     ]
    }
   ],
   "source": [
    "dir_path = '/home/ubuntu/data/longread/full_data/'\n",
    "target_suffix = '.fa'\n",
    "ref_path = '/home/ubuntu/data/longread/proteome/UP000005640_9606.fasta'\n",
    "frames = [1,2,3]\n",
    "results_dir = '/home/ubuntu/data/longread/full_data/blast_results/'\n",
    "scope = 'uniprot'\n",
    "field = 'symbol'\n",
    "species = 'human'\n",
    "n_threads = 4\n",
    "evalue_cutoff = 0.01\n",
    "\n",
    "process_dir_samples (dir_path, \n",
    "                     ref_path,\n",
    "                     target_suffix,\n",
    "                     frames,\n",
    "                     n_threads,\n",
    "                     evalue_cutoff,\n",
    "                     results_dir,\n",
    "                     scope, \n",
    "                     field, \n",
    "                     species\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
